{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R1qckKiUuMN8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1qckKiUuMN8",
        "outputId": "6f005ab6-35aa-420e-f015-9094411beaf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KQutnLWDXVSC",
      "metadata": {
        "id": "KQutnLWDXVSC"
      },
      "outputs": [],
      "source": [
        "FLOOR = 2\n",
        "#Change below to training, testing dataset \n",
        "TRAIN_SET = \"\"\n",
        "TEST_SET = \"\" \n",
        "\n",
        "#Change below to directory for saving model \n",
        "MODEL_DIR = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de1c3b00",
      "metadata": {
        "id": "de1c3b00"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd \n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "963473cd",
      "metadata": {
        "id": "963473cd"
      },
      "source": [
        "## Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86417537",
      "metadata": {
        "id": "86417537"
      },
      "outputs": [],
      "source": [
        "def convert_data_to_tensor(np_data,device):\n",
        "    return torch.tensor(np_data,dtype=torch.float32).to(device)\n",
        "\n",
        "def create_data_loader(np_input, np_label, batch_size, shuffle=False):\n",
        "    device = torch.device('cuda' if (torch.cuda.is_available()) else 'cpu')\n",
        "\n",
        "    input = Variable(convert_data_to_tensor(np_input,device))\n",
        "    label = Variable(convert_data_to_tensor(np_label,device))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        input = input.cuda()\n",
        "        label = label.cuda()\n",
        "\n",
        "    data_loader = DataLoader(dataset=TensorDataset(input,label),\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle)\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "def scale_data(rss_df):\n",
        "    if not('pandas' in str(type(rss_df))):\n",
        "        print(\"ERROR: Please Input Data in pandas.DataFrame\")\n",
        "        return -1\n",
        "\n",
        "    new_rss = rss_df.copy()\n",
        "    new_rss[new_rss == 100] = -110\n",
        "    new_rss[new_rss == -100] = -110\n",
        "\n",
        "    new_rss /= 110\n",
        "    new_rss += 1\n",
        "    return new_rss.values\n",
        "\n",
        "def scale_xy(lon_lat, mean, std):\n",
        "    return (lon_lat - mean) / std\n",
        "\n",
        "def rescale_xy(scale_lon_lat, mean, std):\n",
        "    scale_lon_lat = np.array(scale_lon_lat)\n",
        "    lon = np.transpose(np.array([scale_lon_lat[:, 0]]))\n",
        "    # lon = output[0]\n",
        "    lon_rescaled = lon * std[0] + mean[0]\n",
        "\n",
        "    lat = np.transpose(np.array([scale_lon_lat[:, 1]]))\n",
        "    # lat = output[1]\n",
        "    lat_rescaled = lat * std[1] + mean[1]\n",
        "    return np.array([lon_rescaled, lat_rescaled]).transpose().reshape(-1,2)\n",
        "\n",
        "\n",
        "def save_result (df_pred, df_true, data_name, case_name, save_path):\n",
        "    # % SAVE RESULT\n",
        "    df_result = df_pred.copy()\n",
        "\n",
        "    df_result.join(df_true)\n",
        "    file_path = join(save_path, \"pred_{}_{}.csv\".format(data_name,case_name))\n",
        "    df_result.to_csv(file_path,header=True)\n",
        "\n",
        "\n",
        "    # % Evaluate the Result\n",
        "    eval_euclidean_distance(\n",
        "        df_pred = df_pred,\n",
        "        df_truth = df_true,\n",
        "        dataset_name = data_name,\n",
        "        case_name = case_name,\n",
        "        save_path = save_path,\n",
        "        run_time=-1,\n",
        "        file_name=\"Eval_Results.csv\",\n",
        "        saveFlag=True\n",
        "    )\n",
        "\n",
        "    # Plot - Result\n",
        "    Plot2Dlocation(df_true.values, [[case_name, df_pred.values]], save_path, data_name, case_name,\n",
        "                   connectDots=True, saveFlag=True)\n",
        "\n",
        "    print(\"-- [complete] save pred_{}_{}.csv\".format(data_name,case_name))\n",
        "#end save_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "816c7de5",
      "metadata": {
        "id": "816c7de5"
      },
      "source": [
        "## Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649820d3",
      "metadata": {
        "id": "649820d3"
      },
      "source": [
        "### **Training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2c7d11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac2c7d11",
        "outputId": "703c7931-91c1-4b23-a74b-e3928fbd93c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data shape: torch.Size([13077, 345])\n",
            "label shape: torch.Size([13077, 2])\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(TRAIN_SET)\n",
        "#data = pd.read_csv('/content/drive/MyDrive/FYP/NG/Combined_collections_training_EPSG3414.csv')\n",
        "#data = pd.read_csv('file:/root/bryan/NG/csv-files/filter-train.csv')\n",
        "#label = data[[\"lon_3414\",\"lat_3414\"]]\n",
        "#print(label)\n",
        "label = data[[\"LONGITUDE\",\"LATITUDE\"]]\n",
        "\n",
        "#data = data.drop(columns=['type','timestamp','longitude','latitude','floor_id'])\n",
        "data = data.drop(columns=['TYPE','TIMESTAMP','LONGITUDE','LATITUDE','FLOOR_ID','FLOOR'])\n",
        "#print(data)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Normalize training input data\n",
        "X_train = scale_data(X_train)\n",
        "\n",
        "#Normalize training label: \n",
        "mean = np.mean(y_train.values, axis=0)\n",
        "std = np.std(y_train.values, axis=0)\n",
        "label = scale_xy(y_train.values, mean, std)\n",
        "\n",
        "\n",
        "\n",
        "#convert df to tensor object\n",
        "data = torch.tensor(X_train)\n",
        "\n",
        "label_t = torch.tensor(label)\n",
        "\n",
        "#flatten \n",
        "data_t = torch.flatten(data,1)\n",
        "\n",
        "label_t = torch.flatten(label_t,1)\n",
        "print(\"data shape: \" + str(data_t.shape))\n",
        "print(\"label shape: \" + str(label_t.shape))\n",
        "\n",
        "\n",
        "train_data = [] \n",
        "for i in range(len(data_t)):\n",
        "    train_data.append([data_t[i],label_t[i]])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c38ec5",
      "metadata": {
        "id": "c8c38ec5"
      },
      "source": [
        "### Validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a014906",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a014906",
        "outputId": "acbd84d4-6dad-47e6-e4ff-1f97fb5cb7d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data shape: torch.Size([3270, 345])\n",
            "label shape: torch.Size([3270, 2])\n"
          ]
        }
      ],
      "source": [
        "# data_val = pd.read_csv('file:/root/bryan/NG/csv-files/rfe-test.csv')\n",
        "# label_val=data_val[[\"lon_3414\",\"lat_3414\"]]\n",
        "# data_val=data_val.drop(columns=['type','timestamp','latitude','longitude','lon_3414','lat_3414','floor_id'])\n",
        "\n",
        "\n",
        "#Normalize test input data \n",
        "X_test = scale_data(X_test) \n",
        "\n",
        "#Normalize training label\n",
        "mean_val = np.mean(y_test.values, axis=0)\n",
        "std_val = np.std(y_test.values, axis=0)\n",
        "label_val = scale_xy(y_test.values, mean_val, std_val)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#convert df to tensor object\n",
        "data_val_t1 = torch.tensor(X_test)\n",
        "\n",
        "label_val_t = torch.tensor(label_val)\n",
        "\n",
        "#flatten \n",
        "data_val_t = torch.flatten(data_val_t1,1)\n",
        "\n",
        "label_val_t = torch.flatten(label_val_t,1)\n",
        "print(\"data shape: \" + str(data_val_t.shape))\n",
        "print(\"label shape: \" + str(label_val_t.shape))\n",
        "\n",
        "val_data = [] \n",
        "for i in range(len(data_val_t)):\n",
        "    val_data.append([data_val_t[i],label_val_t[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5955e270",
      "metadata": {
        "id": "5955e270"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "56099827",
      "metadata": {
        "id": "56099827"
      },
      "source": [
        "### Put into train/test loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11acae4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e11acae4",
        "outputId": "857f5692-985d-4d45-976a-c08be49b00ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finish dataset part\n"
          ]
        }
      ],
      "source": [
        "#put both train and validation sets into loader \n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
        "testloader = torch.utils.data.DataLoader(val_data, batch_size=32)\n",
        "\n",
        "print(\"Finish dataset part\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18f3b531",
      "metadata": {
        "id": "18f3b531"
      },
      "source": [
        "## Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2602653a",
      "metadata": {
        "id": "2602653a"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(data_val_t.shape[1],300)\n",
        "        self.fc2 = nn.Linear(300,300)\n",
        "        self.fc3 = nn.Linear(300,300)\n",
        "        self.fc4 = nn.Linear(300,2)\n",
        "\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790e0015",
      "metadata": {
        "id": "790e0015"
      },
      "outputs": [],
      "source": [
        "def weight_reset(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        m.reset_parameters()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a33f2196",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a33f2196",
        "outputId": "666e4c97-097e-4982-ab89-01330cef0246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computation device: cpu\n",
            "\n",
            "Epoch:1/500.. Training Loss:0.016.. Test Loss:0.002..\n",
            "Epoch:2/500.. Training Loss:0.009.. Test Loss:0.002..\n",
            "Epoch:3/500.. Training Loss:0.008.. Test Loss:0.002..\n",
            "Epoch:4/500.. Training Loss:0.007.. Test Loss:0.002..\n",
            "Epoch:5/500.. Training Loss:0.007.. Test Loss:0.002..\n",
            "Epoch:6/500.. Training Loss:0.007.. Test Loss:0.002..\n",
            "Epoch:7/500.. Training Loss:0.006.. Test Loss:0.002..\n",
            "Epoch:8/500.. Training Loss:0.006.. Test Loss:0.002..\n",
            "Epoch:9/500.. Training Loss:0.006.. Test Loss:0.002..\n",
            "Epoch:10/500.. Training Loss:0.006.. Test Loss:0.002..\n",
            "Epoch:11/500.. Training Loss:0.006.. Test Loss:0.002..\n",
            "Epoch:12/500.. Training Loss:0.006.. Test Loss:0.002..\n",
            "Epoch:13/500.. Training Loss:0.006.. Test Loss:0.002..\n",
            "Epoch:14/500.. Training Loss:0.006.. Test Loss:0.001..\n",
            "Epoch:15/500.. Training Loss:0.006.. Test Loss:0.001..\n",
            "Epoch:16/500.. Training Loss:0.006.. Test Loss:0.001..\n",
            "Epoch:17/500.. Training Loss:0.005.. Test Loss:0.002..\n",
            "Epoch:18/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:19/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:20/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:21/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:22/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:23/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:24/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:25/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:26/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:27/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:28/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:29/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:30/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:31/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:32/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:33/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:34/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:35/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:36/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:37/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:38/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:39/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:40/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:41/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:42/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:43/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:44/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:45/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:46/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:47/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:48/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:49/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:50/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:51/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:52/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:53/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:54/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:55/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:56/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:57/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:58/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:59/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:60/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:61/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:62/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:63/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:64/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:65/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:66/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:67/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:68/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:69/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:70/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:71/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:72/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:73/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:74/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:75/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:76/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:77/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:78/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:79/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:80/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:81/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:82/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:83/500.. Training Loss:0.005.. Test Loss:0.001..\n",
            "Epoch:84/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:85/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:86/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:87/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:88/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:89/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:90/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:91/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:92/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:93/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:94/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:95/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:96/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:97/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:98/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:99/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:100/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:101/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:102/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:103/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:104/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:105/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:106/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:107/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:108/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:109/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:110/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:111/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:112/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:113/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:114/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:115/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:116/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:117/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:118/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:119/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:120/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:121/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:122/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:123/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:124/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:125/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:126/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:127/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:128/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:129/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:130/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:131/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:132/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:133/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:134/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:135/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:136/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:137/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:138/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:139/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:140/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:141/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:142/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:143/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:144/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:145/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:146/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:147/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:148/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:149/500.. Training Loss:0.004.. Test Loss:0.001..\n",
            "Epoch:150/500.. Training Loss:0.004.. Test Loss:0.001..\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bb531a827fb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "LR = 0.0001\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Computation device: {device}\\n\")\n",
        "model = Classifier()\n",
        "model.apply(weight_reset)\n",
        "model.to(device)\n",
        "#optimizer = optim.Adam(model.parameters(), lr= LR, eps = 1e-9) \n",
        "optimizer = optim.Adam(model.parameters(), lr= LR) \n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr = 0.001, weight_decay=0.03, momentum=0.003)\n",
        "#optimizer = optim.SGD(model.parameters(), lr = LR)\n",
        "criterion = nn.MSELoss() #MSE\n",
        "#criterion = torch.nn.L1Loss()\n",
        "\n",
        "epochs = 500\n",
        "train_losses, test_losses = [],[]\n",
        "\n",
        "for e in range(epochs):\n",
        "    tot_train_loss = 0 \n",
        "    for data, labels in trainloader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        data = data.view(data.shape[0],-1)\n",
        "        data = data.float()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        #print(output)\n",
        "        #output = ((output - torch.mean(output))/torch.max(output)-torch.min(output))\n",
        "        loss = torch.sqrt(criterion(output,labels.float())) #RMSE \n",
        "        #loss = criterion(output,labels.float())\n",
        "        tot_train_loss+=loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "\n",
        "    else:\n",
        "        #if e>=100 and e%5==0 :\n",
        "            #torch.save(model.state_dict(), './outputs/dec_control/dec-epoch-{}.pth'.format(e))\n",
        "        tot_test_loss = 0 \n",
        "        with torch.no_grad():\n",
        "            for data, labels in testloader:\n",
        "                data, labels = data.to(device), labels.to(device)\n",
        "                data = data.view(data.shape[0],-1)\n",
        "                data=data.float()\n",
        "                test_output = model(data)\n",
        "                loss = torch.sqrt(criterion(test_output,labels.float()))\n",
        "                #loss = criterion(test_output,labels.float())\n",
        "\n",
        "                tot_test_loss+=loss.item()\n",
        "                \n",
        "        train_loss = tot_train_loss/len(trainloader.dataset)\n",
        "        test_loss = tot_test_loss/len(trainloader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(\"Epoch:{}/{}..\".format(e+1,epochs),\n",
        "             \"Training Loss:{:.3f}..\".format(train_loss),\n",
        "             \"Test Loss:{:.3f}..\".format(test_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nVx6pKjayj0u",
      "metadata": {
        "id": "nVx6pKjayj0u"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# loss plots\n",
        "plt.figure(figsize=(16,9))\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
        "\n",
        "ax1.plot(train_losses, color='orange', label='train loss',)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "ax1.legend()\n",
        "\n",
        "# loss plots\n",
        "ax2.plot(test_losses, color='red', label='validation loss')\n",
        "ax2.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0690a3a1",
      "metadata": {
        "id": "0690a3a1"
      },
      "source": [
        "## Evaluate on test set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45382b74",
      "metadata": {
        "id": "45382b74"
      },
      "outputs": [],
      "source": [
        "#validation_loss, validation_pred = evaluate(data_val_t, label_val_t, model, criterion, mean, std)\n",
        "\n",
        "data_val_new = pd.read_csv(TEST_SET)\n",
        "# data_val_new = pd.read_csv('/content/drive/MyDrive/FYP/NG/Combined_collections_validation_EPSG3414.csv')\n",
        "#data_val_new = pd.read_csv('file:/root/bryan/NG/csv-files/filter-test.csv')\n",
        "\n",
        "# label_val_new = data_val_new[[\"lon_3414\",\"lat_3414\"]]\n",
        "# data_val_new = data_val_new.drop(columns=['type','timestamp','longitude','latitude','floor_id'])\n",
        "\n",
        "\n",
        "label_val_new=data_val_new[[\"LONGITUDE\",\"LATITUDE\"]]\n",
        "data_val_new=data_val_new.drop(columns=['TYPE','TIMESTAMP','LONGITUDE','LATITUDE','FLOOR_ID','FLOOR'])\n",
        "\n",
        "\n",
        "def evaluate(test_input, test_label, model, loss_fn, xy_mean, xy_std):\n",
        "    model.eval()\n",
        "    pred = model(convert_data_to_tensor(scale_data(test_input), device))\n",
        "    val_loss = loss_fn(\n",
        "        convert_data_to_tensor(rescale_xy(pred.data.cpu().numpy(), xy_mean, xy_std), device),\n",
        "        convert_data_to_tensor(test_label, device)).data\n",
        "    return val_loss.item(), rescale_xy(pred.data.cpu().numpy(), xy_mean, xy_std)\n",
        "\n",
        "validation_loss, validation_pred = evaluate(data_val_new,label_val_new.values, model, criterion, mean, std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NWXaOKUeSZt1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWXaOKUeSZt1",
        "outputId": "ba56966f-8005-4c79-cd8d-2ef353f8297b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[30032.69349262 30317.32534712]\n",
            "[22.18997454 36.97402449]\n"
          ]
        }
      ],
      "source": [
        "print(mean)\n",
        "print(std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f5f908",
      "metadata": {
        "id": "c7f5f908"
      },
      "outputs": [],
      "source": [
        "print(\"Validation loss: \" + str('{0:.2f}'.format(validation_loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "560c851b",
      "metadata": {
        "id": "560c851b"
      },
      "outputs": [],
      "source": [
        "GRAPH_COLOUR = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
        "MARKER = ['o', '.', ',', 'x', '+', 'v', '^', '<', '>', 's', 'd']\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "x,y = label_val_new.values.T\n",
        "plt.scatter(x,y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e13da2",
      "metadata": {
        "id": "88e13da2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "x,y = validation_pred.T\n",
        "plt.scatter(x,y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f5f820",
      "metadata": {
        "id": "c4f5f820"
      },
      "outputs": [],
      "source": [
        "dist_list = [] \n",
        "for i in range(len(validation_pred)):\n",
        "    dist = np.linalg.norm(validation_pred[i]-label_val_new.values[i])\n",
        "    dist_list.append(dist)\n",
        "    \n",
        "print(\"Validation loss: \" + str('{0:.2f}'.format(validation_loss)))\n",
        "print(\"Max: \" + str('{0:.2f}'.format(max(dist_list))) + \" m\")\n",
        "print(\"Min: \" + str('{0:.2f}'.format(min(dist_list))) + \" m\")\n",
        "print(\"Average: \" + str('{0:.2f}'.format(np.average(dist_list))) + \" m\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc7c68fc",
      "metadata": {
        "id": "fc7c68fc"
      },
      "outputs": [],
      "source": [
        "def Plot2Dlocation(np_xy_truth, list_xy_preds,result_path, dataset_name, plot_name, connectDots=True,saveFlag=False):\n",
        "    \"\"\"\n",
        "    Plot the 2D map with the ground truth and prediction results (list_xy_preds could hold 1 set of results or more)\n",
        "    :param main_info (HouseKeeping):\n",
        "    :param np_xy_truth (Numpy):\n",
        "    :param list_xy_preds (List<String,2d_np>): e.g. list_xy_preds=[[\"Euclidean\",df_pred.values]]\n",
        "    :param dataset_name (String):\n",
        "    :param plot_name (String):\n",
        "    :param connectDots (Bool):\n",
        "    :param saveFlag (Bool):\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    plt.clf()   #clear the plot\n",
        "    fig, ax = plt.subplots(figsize=(16, 10))\n",
        "    #Figure Info\n",
        "    plt.title('{}\\n({})'.format(plot_name,dataset_name))\n",
        "    #plt.title(dataset_name,y=1)\n",
        "    #plt.suptitle(plot_name, y=1.1, fontsize=18)\n",
        "    #plt.title(dataset_name, fontsize=10)\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "\n",
        "    #plot groundtruth\n",
        "    plt.plot(np_xy_truth[:,0], np_xy_truth[:,1], GRAPH_COLOUR[0]+MARKER[0],label=\"Ground Truth\")\n",
        "\n",
        "    #connect the points\n",
        "    for caseNum in range(len(list_xy_preds)):\n",
        "        #plot pred point (scatter)\n",
        "        #plt.plot(list_xy_preds[caseNum][1][:, 0], list_xy_preds[caseNum][1][:, 1], GRAPH_COLOUR[caseNum+1] + MARKER[caseNum+1],label=list_xy_preds[caseNum][0])\n",
        "        plt.scatter(list_xy_preds[caseNum][1][:, 0], list_xy_preds[caseNum][1][:, 1],c=GRAPH_COLOUR[caseNum+1],s=5,label=list_xy_preds[caseNum][0])\n",
        "\n",
        "        if (connectDots):\n",
        "            x = np.array([np_xy_truth[:, 0], list_xy_preds[caseNum][1][:, 0]])\n",
        "            y = np.array([np_xy_truth[:, 1], list_xy_preds[caseNum][1][:, 1]])\n",
        "            plt.plot(x, y, GRAPH_COLOUR[caseNum + 1], alpha=0.2)\n",
        "\n",
        "    plt.legend()\n",
        "    if saveFlag:\n",
        "        saveName = join(result_path,dataset_name+\"_\"+plot_name+\".png\")\n",
        "        plt.savefig(saveName)\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "    #endFunction Plot2Dlocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-0LO6kBnH9hG",
      "metadata": {
        "id": "-0LO6kBnH9hG"
      },
      "outputs": [],
      "source": [
        "temp_pred = [] \n",
        "for i in range(len(validation_pred)):\n",
        "  temp_pred.append(validation_pred[i])\n",
        "df_pred = pd.DataFrame(temp_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2K85oeoMIJzM",
      "metadata": {
        "id": "2K85oeoMIJzM"
      },
      "outputs": [],
      "source": [
        "print((label_val_new))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xnpnaCkcGc-A",
      "metadata": {
        "id": "xnpnaCkcGc-A"
      },
      "outputs": [],
      "source": [
        "Plot2Dlocation(label_val_new.values, [[\"Euclidean\",df_pred.values]],\"test\",\"National Gallery\",\"All floors combined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9WwBqG6FMpTA",
      "metadata": {
        "id": "9WwBqG6FMpTA"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), MODEL_DIR)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NG_DNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
